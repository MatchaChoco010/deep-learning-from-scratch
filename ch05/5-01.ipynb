{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 単純なレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 乗算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        \n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.00000000000003\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2 110.00000000000001 200\n"
     ]
    }
   ],
   "source": [
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = x_3 \\times (x_1 \\times x_2)$$\n",
    "$$x_1 = 100, x_2 = 2, x_3 = 1.1$$\n",
    "$$y = x_3 \\times t$$\n",
    "$$t = x_1 \\times x_2$$\n",
    "$$\\frac{\\partial y}{\\partial t} = x_3$$\n",
    "$$\\frac{\\partial t}{\\partial x_1} = x_2$$\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x_1} = \\frac{\\partial y}{\\partial t}\\frac{\\partial t}{\\partial x_1}\\\\\n",
    "= x_3 \\times x_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とりあえず$x_1$での微分を式にしてみた。\n",
    "$x_3$での微分を考えてみると、$(x_1 \\times x_2)$がその値になる。\n",
    "順伝播のときに値をキャプチャしておけば、それがそのまま使えるということか。\n",
    "数式でイメージすると、微分を計算してから実際の値を代入して実際の微分の値を計算するイメージだけど。\n",
    "いきなり実際の値を当てはめながら計算していけるのか。\n",
    "なんか騙されたような釈然としない用な感覚だ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加算レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "715.0000000000001\n",
      "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# foward\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# backward\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(price)\n",
    "print(dapple_num, dapple, dorange, dorange_num, dtax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 活性化関数レイヤの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPyの配列が入ってくる前提だけれども、各配列のセルごとに計算をしているだけで他のセルは関係しないからまだ簡単だね。\n",
    "\n",
    "加算や乗算などは二項演算だったけれどもこれは単項のみだからとてもシンプル。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLUレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoidレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1}{1+\\exp(-x)}$$\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = -\\frac{1}{(1 + \\exp(-x))^2} \\cdot \\exp(-x) \\cdot (-1)\\\\\n",
    "= \\frac{1}{1+\\exp(-x)} \\cdot \\frac{\\exp(-x)}{1+\\exp(-x)}\\\\\n",
    "= \\frac{1}{1+\\exp(-x)} \\cdot (1 - \\frac{1}{1+\\exp(-x)})\\\\\n",
    "= y(1-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "でかい計算グラフの中ほどにSigmoidがあった場合は、逆伝播で$y(1-y)$をかけ合わせて上流に流せば良い。\n",
    "$y$はSigmoidノードの出力なので、順伝播のときにSigmoidの出力を記憶しておけば、逆伝播でその出力を使った$y(1-y)$をかけ合わせて上に流せば良い。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        seelf.out = out\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.out * (1.0 - self.out)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affine/Softmaxレイヤの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数の手前の重み付き加算とバイアスの加算について。\n",
    "入力のベクトルに重みの行列をかけ合わせたベクトルにさらにバイアスのベクトルを足し合わせる。\n",
    "それをシグモイドなりなんなりの活性化関数にかける。\n",
    "\n",
    "バイアスは重みと違って張り巡らされたネットワークではなくその後の一律の加算の部分だね。だから行列で例えば2個のノードから3個のノードへの値を持つ重み行列とは違って、その後の3個に対してバイアスを持てば良いのでバイアスはベクトルになっている。\n",
    "\n",
    "![memo](memo.jpg)\n",
    "\n",
    "この図でいうと活性化関数は単項演算に当たるのでパラメータは無いということになるのかな。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数のelement-wiseな計算とは違って、ここでは行列の積が出てくる。\n",
    "この行列の内積演算について微分を行わなければならない。\n",
    "\n",
    "本では計算グラフのノード間を流れる値を行列に拡張して単純なグラフで表している。\n",
    "上の画像のような細かいノードの結びつきではなくて行列の積をそのまま扱う形で説明しているね。\n",
    "\n",
    "この部分のグラフについては\n",
    "$$Y = X \\cdot W + B$$\n",
    "という計算式になる。\n",
    "\n",
    "$$Y = T + B$$\n",
    "$$T = X \\cdot W$$\n",
    "ということになるのかな。\n",
    "\n",
    "$\\frac{\\partial L}{\\partial X}$と$\\frac{\\partial L}{\\partial W}$を求めたい。$L$は全体の計算グラフ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本によれば\n",
    "$$\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y} \\cdot W^{\\mathrm{T}}$$\n",
    "$$\\frac{\\partial L}{\\partial W} = X^{\\mathrm{T}} \\cdot \\frac{\\partial L}{\\partial Y}$$\n",
    "だそうだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "行列での内積の連鎖律がこうなるということなのかな。\n",
    "本では細かい解説が省かれているので実際に計算してみるか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Y = f(X \\cdot W)$$\n",
    "とする。\n",
    "\n",
    "$$Y = f(T)$$\n",
    "$$T = X \\cdot W$$\n",
    "ということになる。\n",
    "\n",
    "ここで$f$はelement-wiseな計算ね。\n",
    "$$f(T) \\equiv \\left(\n",
    "    \\begin{matrix}\n",
    "        f(t_{11}) & \\ldots & f(t_{n1})\\\\\n",
    "        \\vdots & \\ddots & \\vdots\\\\\n",
    "        f(t_{1m}) & \\ldots & f(t_{nm})\n",
    "    \\end{matrix}\n",
    "\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2x2で手計算したらそれらしくなったけれども、なにかおかしい気がする。内積したあとで微分するならば正しくなるけれども、微分したのを内積したらだめじゃね？\n",
    "\n",
    "連鎖率についてきちんと勉強しないとだめだ。1変数のときは合成関数の微分で簡単だけれども。変数が複数になるとそうは行かないようだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学部時代のノートを引っ張り出してきてみてみたけれども、行列を使わずに計算しているので計算式がだいぶ違ってあれだ。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [誤差逆伝播法をはじめからていねいに - Qiita](https://qiita.com/43x2/items/50b55623c890564f1893)\n",
    "\n",
    "こちらにこの部分に関する式展開があった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バッチ版Affineレイヤ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力をまとめて行列にしてしまうのだったね。\n",
    "バイアスの加算にはブロードキャストが行われている。\n",
    "\n",
    "次はバッチサイズが2個だった場合。\n",
    "入力が3次のベクトルが2つになって、行列の積が行われる。その結果にバイアスをブロードキャストで足していく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n",
    "B = np.array([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0],\n",
       "       [10, 10, 10]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [11, 12, 13]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dot_W + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バイアスの逆伝播はバッチの数だけ合算したものと考えることになるらしい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "dY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dB = np.sum(dY, axis=0)\n",
    "dB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先に実装した活性化関数はバッチにしてもうまく働くね。なにしろセル一つに対して実行すれば良くて、NumPyでelement-wiseに作られている。\n",
    "\n",
    "このアフィンレイヤはelement-wiseで処理すれば良い活性化関数とは異なってくるので注意が必要ということに。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(selg, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax-with-Lossレイヤ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        \n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
